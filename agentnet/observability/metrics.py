"""
Enhanced, production-ready Prometheus Metrics Collection for AgentNet.

This module provides a robust, singleton-based metrics collection system with
richly labeled metrics for deep observability into agent performance, cost,
errors, and behavior.
"""

import logging
import time
from contextlib import contextmanager
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional

logger = logging.getLogger("agentnet.observability.metrics")

try:
    from prometheus_client import (
        CollectorRegistry,
        Counter,
        Gauge,
        Histogram,
        start_http_server,
    )
    PROMETHEUS_AVAILABLE = True
except ImportError:
    logger.warning("prometheus_client not available - metrics will be logged as debug messages.")
    PROMETHEUS_AVAILABLE = False

    # --- Enhanced Mock Classes for Testing and Fallback ---
    @dataclass
    class MockMetric:
        name: str
        _last_labels: Dict[str, Any] = field(default_factory=dict)
        _last_value: float = 0.0

        def labels(self, **kwargs):
            self._last_labels = kwargs
            return self

        def inc(self, amount=1):
            self._last_value = amount
            logger.debug(f"METRIC_INC: {self.name}{self._last_labels} by {amount}")

        def observe(self, value):
            self._last_value = value
            logger.debug(f"METRIC_OBSERVE: {self.name}{self._last_labels} value={value}")

        def set(self, value):
            self._last_value = value
            logger.debug(f"METRIC_SET: {self.name}{self._last_labels} to {value}")

    Counter = Histogram = Gauge = MockMetric
    CollectorRegistry = object

    def start_http_server(port, registry=None):
        logger.info(f"Mock Prometheus server would start on port {port}. Metrics will be logged.")


@dataclass
class MetricValue:
    """A simple data class for holding a metric value when Prometheus is not available."""
    name: str
    value: float
    labels: Dict[str, str]
    timestamp: datetime


class AgentNetMetrics:
    """
    Centralized, singleton-aware metrics collection for AgentNet operations.
    """
    _instance: Optional["AgentNetMetrics"] = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(AgentNetMetrics, cls).__new__(cls)
        return cls._instance

    def __init__(
        self,
        registry: Optional[CollectorRegistry] = None,
        enable_server: bool = False,
        port: int = 8000,
    ):
        # Prevent re-initialization of the singleton
        if hasattr(self, "_initialized") and self._initialized:
            return
        
        self.registry = registry or (CollectorRegistry() if PROMETHEUS_AVAILABLE else None)
        self.enable_prometheus = PROMETHEUS_AVAILABLE
        self._local_metrics_log: List[MetricValue] = []

        if self.enable_prometheus:
            self._setup_prometheus_metrics()
            if enable_server:
                try:
                    start_http_server(port, registry=self.registry)
                    logger.info(f"Prometheus metrics server started on http://localhost:{port}")
                except OSError as e:
                    logger.error(f"Could not start Prometheus server on port {port}: {e}. Server may already be running.")
        
        self._initialized = True

    def _setup_prometheus_metrics(self):
        """Initialize all Prometheus metric objects with rich labels."""
        # Latency buckets in seconds, from 100ms to 30s
        latency_buckets = (0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, float("inf"))
        
        self.inference_latency = Histogram(
            "agentnet_inference_latency_seconds", "Agent inference duration.",
            ["model", "provider", "agent", "status"], buckets=latency_buckets, registry=self.registry
        )
        self.tokens_input_total = Counter(
            "agentnet_tokens_input_total", "Input tokens consumed by an agent.",
            ["model", "provider", "tenant", "agent"], registry=self.registry
        )
        self.tokens_output_total = Counter(
            "agentnet_tokens_output_total", "Output tokens generated by an agent.",
            ["model", "provider", "tenant", "agent"], registry=self.registry
        )
        self.violations_total = Counter(
            "agentnet_violations_total", "Policy violations detected by monitors.",
            ["severity", "monitor_name", "agent"], registry=self.registry
        )
        self.cost_usd_total = Counter(
            "agentnet_cost_usd_total", "Estimated cost in USD.",
            ["provider", "model", "tenant", "agent"], registry=self.registry
        )
        self.session_rounds_total = Counter(
            "agentnet_session_rounds_total", "Session rounds completed.",
            ["mode", "converged"], registry=self.registry
        )
        self.tool_invocations_total = Counter(
            "agentnet_tool_invocations_total", "Tool invocations.",
            ["tool_name", "status", "agent"], registry=self.registry
        )
        self.dag_node_duration_seconds = Histogram(
            "agentnet_dag_node_duration_seconds", "DAG node execution duration.",
            ["agent", "node_type", "status"], buckets=latency_buckets, registry=self.registry
        )
        self.exceptions_total = Counter(
            "agentnet_exceptions_total", "Unhandled exceptions.",
            ["exception_type", "location"], registry=self.registry
        )
        self.active_sessions = Gauge(
            "agentnet_active_sessions", "Currently active sessions.", registry=self.registry
        )
        logger.info("Prometheus metrics initialized.")

    def _record(self, name: str, value: float, labels: Dict[str, str]):
        """Internal helper to log metrics when Prometheus is unavailable."""
        if not self.enable_prometheus:
            metric = MetricValue(name=name, value=value, labels=labels, timestamp=datetime.now())
            self._local_metrics_log.append(metric)
            if len(self._local_metrics_log) > 2000: # Memory guard
                self._local_metrics_log = self._local_metrics_log[-1000:]

    # --- Public Metric Recording Methods ---

    def record_inference_latency(self, duration_s: float, model: str, provider: str, agent: str, status: str = "success"):
        self.inference_latency.labels(model=model, provider=provider, agent=agent, status=status).observe(duration_s)
        self._record("inference_latency_seconds", duration_s, {"model": model, "provider": provider, "agent": agent, "status": status})

    def record_tokens(self, in_tokens: int, out_tokens: int, model: str, provider: str, agent: str, tenant: str = "default"):
        self.tokens_input_total.labels(model=model, provider=provider, tenant=tenant, agent=agent).inc(in_tokens)
        self.tokens_output_total.labels(model=model, provider=provider, tenant=tenant, agent=agent).inc(out_tokens)
        self._record("tokens_input_total", in_tokens, {"model": model, "provider": provider, "tenant": tenant, "agent": agent})
        self._record("tokens_output_total", out_tokens, {"model": model, "provider": provider, "tenant": tenant, "agent": agent})

    def record_violation(self, severity: str, monitor_name: str, agent: str):
        self.violations_total.labels(severity=severity, monitor_name=monitor_name, agent=agent).inc()
        self._record("violations_total", 1, {"severity": severity, "monitor_name": monitor_name, "agent": agent})

    def record_cost(self, cost_usd: float, provider: str, model: str, agent: str, tenant: str = "default"):
        self.cost_usd_total.labels(provider=provider, model=model, tenant=tenant, agent=agent).inc(cost_usd)
        self._record("cost_usd_total", cost_usd, {"provider": provider, "model": model, "tenant": tenant, "agent": agent})

    def record_session_round(self, mode: str, converged: bool):
        converged_str = "true" if converged else "false"
        self.session_rounds_total.labels(mode=mode, converged=converged_str).inc()
        self._record("session_rounds_total", 1, {"mode": mode, "converged": converged_str})

    def record_tool_invocation(self, tool_name: str, status: str, agent: str):
        self.tool_invocations_total.labels(tool_name=tool_name, status=status, agent=agent).inc()
        self._record("tool_invocations_total", 1, {"tool_name": tool_name, "status": status, "agent": agent})

    def record_dag_node_duration(self, duration_s: float, agent: str, node_type: str, status: str = "success"):
        self.dag_node_duration_seconds.labels(agent=agent, node_type=node_type, status=status).observe(duration_s)
        self._record("dag_node_duration_seconds", duration_s, {"agent": agent, "node_type": node_type, "status": status})

    def record_exception(self, exception: Exception, location: str):
        exc_type = type(exception).__name__
        self.exceptions_total.labels(exception_type=exc_type, location=location).inc()
        self._record("exceptions_total", 1, {"exception_type": exc_type, "location": location})

    def set_active_sessions(self, count: int):
        self.active_sessions.set(count)
        self._record("active_sessions", count, {})

    # --- Smart Context Managers ---

    @contextmanager
    def measure_inference(self, model: str, provider: str, agent: str):
        start_time = time.monotonic()
        status = "success"
        try:
            yield
        except Exception as e:
            status = "error"
            self.record_exception(e, location=f"inference:{agent}")
            raise
        finally:
            duration = time.monotonic() - start_time
            self.record_inference_latency(duration, model, provider, agent, status)

    @contextmanager
    def measure_dag_node(self, agent: str, node_type: str):
        start_time = time.monotonic()
        status = "success"
        try:
            yield
        finally:
            duration = time.monotonic() - start_time
            self.record_dag_node_duration(duration, agent, node_type, status)


# --- Global Singleton Management ---

_global_metrics_instance: Optional[AgentNetMetrics] = None

def configure_global_metrics(**kwargs):
    """
    Configure and initialize the global metrics singleton instance.
    Should be called once at application startup.

    Args:
        registry: Optional Prometheus registry.
        enable_server (bool): If True, starts the Prometheus HTTP server.
        port (int): Port for the HTTP server.
    """
    global _global_metrics_instance
    if _global_metrics_instance is None:
        _global_metrics_instance = AgentNetMetrics(**kwargs)
    return _global_metrics_instance

def get_global_metrics() -> AgentNetMetrics:
    """
    Get the configured global metrics instance.
    Initializes a default instance if not already configured.
    """
    global _global_metrics_instance
    if _global_metrics_instance is None:
        logger.warning("Global metrics not configured. Initializing with default settings.")
        _global_metrics_instance = AgentNetMetrics()
    return _global_metrics_instance
